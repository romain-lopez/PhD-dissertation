\section{Discussion}

% intro
Our study focuses on an important need in the field of single-cell RNA-seq -- namely, a method capable of accounting for confounding factors and measurement uncertainty in tertiary analysis tasks (e.g., clustering, differential expression, and annotation) through a common, scalable statistical model. To achieve this, we developed scVI -- a hierarchical Bayesian model that makes use of neural networks to provide a complete probabilistic representation of single-cell transcriptomes. In this study, we demonstrated that scVI provides a computationally efficient and ``all-inclusive'' approach to denoising and analyzing gene expression data, and showcased its performance by comparing it to the state-of-the-art methods for a range of downstream tasks, including dimensionality reduction, imputation, visualization, batch-effect removal, clustering and differential expression.  

The scVI procedure takes as input a matrix of counts, and therefore does not need a preliminary normalization step. Instead, it learns a cell-specific scaling factor as a hidden variable of the model, with the objective of maximizing the likelihood of the data (as in~\cite{biscuit,zinbwave,basics}), which is more justifiable than \emph{a posteriori} correction of the observed counts~\cite{vallejos2017normalizing}. Further, scVI explicitly accounts for the contribution of discrete nuisance factors, such as batch annotations, by enforcing conditional independence between them and the (inferred) parameters that govern gene expression distributions. We demonstrate that these normalization components are needed to achieve good performance in critical tasks such as clustering and differential expression (Figures \ref{scvibatch_panel}, \ref{scvibayes_panel}, \ref{scviclustering_supp}, and \ref{scviMNNfigure}). Notably, the batch correction step is performed via the mild modeling assumption of conditional independence, and therefore scVI is expected to reasonably integrate and harmonize multiple datasets. Further modeling would be needed for more intensive usage of batch removal (number of batches/datasets $\geq 20$) and is left as an avenue for future research. 

%NIR: new section. Please read
%The support provided by scVI to a range of important analysis tasks and the inclusion of nuisance factors in the model distinguish it from other recent applications of neural networks for scRNA-seq analysis~\cite{scvis,VASC,dca,scVAE} (Methods section~\ref{scvirelated_work}). For instance, DCA can only address a subset of the tasks addressed by scVI (Table~\ref{scvialgorithms_pres}). Indeed, DCA performs comparably to scVI at the tasks it was designed for, including imputation (Figures S1 and S3), and clustering a single batch (Figure~\ref{scviMNNfigure}). However, this is not the case for differential expression, possibly due to the need of an external model~\cite{deseq2} for that purpose (since DCA does not define a posterior over the inferred parameters; Figure~\ref{scvibayes_panel}). More critically, we find that DCA may fail to align data from different batches (Figure \ref{scviclustering_supp}, and \ref{scviMNNfigure}), demonstrating that explicitly accounting for batch effects is an important distinguishing feature of scVI, especially provided that many scRNA-seq studies consist of multiple batches.

Another important feature of scVI and other methods based on neural networks~\cite{scvis,VASC,dca,scVAE} is their scalability. Indeed, unlike many of the benchmark methods, scVI is capable of efficiently processing the very large datasets of up to a million cells explored in this study~\cite{Regev2017,10x}. To achieve this high level of scalability while ensuring a good fit to the data, we designed an efficient procedure to learn the parameters of our graphical model. Importantly, exact Bayesian inference is in most cases not tractable for these kinds of models. Furthermore, until recently, even variational inference was rarely applied to such models without restrictive ``conditional conjugacy'' properties. To address this, we use a stochastic optimization procedure that samples our approximation of the posterior distribution (as well as random subsamples or ``mini-batches'' of our dataset), allowing us to efficiently perform inference with arbitrary models, including those with conditional distributions specified by neural networks~\cite{kingma2013}.

 

The deep learning architecture used in scVI is built on several canonical building blocks such as rectified linear units, fully-connected layers, dropout~\cite{Srivastava2014}, mini-batch normalization~\cite{Ioffe2015}, deterministic warm up~\cite{SÃ¸nderby2016}, and mean-field approximation to the posterior~\cite{kingma2013}. Together, these building blocks provide the means to effectively fit the generative model of scVI and approximate its posterior. However, an important area of research going forward is to explore other, possibly better, architectures~\cite{Zoph} and procedures for parameter and hyper-parameter tuning~\cite{Bergstra2011}, which may in some instances increase the accuracy of the inferred model. Notably, since our procedure has a random component, and since it optimizes a non-convex objective function, it may give alternative results with different initializations. To address this concern, we demonstrate the stability of scVI in terms of its objective function, as well as imputation and clustering (Figure~\ref{scvirobustness}). Another related issue is that, if there are few observations (cells) for each gene, the prior (and the inductive bias of the neural network) may keep us from fitting the data closely. Indeed, in the case of datasets such as HEMATO~\cite{Tusi2018}, where the number of cells is smaller than the number of genes, some procedure to pre-filter the genes may be warranted. Another approach that could help make scVI applicable to smaller data sets (hundreds of cells), which we intend to explore, is utilizing techniques such as Bayesian shrinkage~\cite{deseq2} or regularization and second order optimization with larger mini-batch size / full dataset ~\cite{zinbwave}. We do however show that for a range of datasets of varying sizes, scVI is able to fit the data well  and capture relevant biological diversity between cells.
%and is capable of producing meaningful downstream analysis.
%For review: stability of \rho values

From a system perspective, single-cell RNA-seq analyses paradoxically benefit from the abundance of zero values, as this allows us to store the data in a sparse (rather than dense) matrix format. A sparse matrix with one million cells and ten thousand genes would represent around 7.5 GB, assuming one percent fill. On the other hand, the output of batch- corrected data is not sparse, and is therefore potentially very large (for 1M cells, approximately 75 GB). While it performs batch correction, scVI still provides a compact representation of the complete data, as it requires only the latent space and the specification of the model (overall, it has a memory footprint of less than 1G, assuming 10 latent variables). One obvious drawback of such compressed representation (apart from the potential loss of information) is that gene expression values need to be computed on the fly; however, this can be done very efficiently in a single pass through the generative networks (which requires approximately 10 seconds for generating the $\rho$ matrix for a test dataset of 500k cells and 8k genes, with the same hardware specification used throughout this chapter). This property makes scVI a good baseline for use in interactive visualization tools~\cite{DeTomaso2016, Fan2016, Wolf2018}.

Looking ahead, it is important to note that the model of scVI is very general and therefore provides a proper statistical framework for other forms of scRNA-seq analysis that were not explored in this chapter, such as lineage inference~\cite{Semrau2017} or cell-state annotation~\cite{tanay2017scaling,wagner2016revealing}. Further, as the scale and diversity of single-cell RNA-seq increase, we expect that there will be a great demand for tools such as scVI, especially in cases where there is interest in harmonizing datasets in a manner that is scalable and conducive to various forms of downstream analysis~\cite{Regev2017}. Indeed, one subsequent research direction would be to merge multiple datasets from a given tissue to build a generative model with biological annotations of cell-types or phenotypical conditions in a semi-supervised fashion. This would allow researchers to ``query'' the generative model with a new dataset in an online fashion in order to ``retrieve'' previous biological information, which would allow verification of reproducibility across experiments, as well as transfer of cell state annotations between studies.

