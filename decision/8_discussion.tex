\section{Discussion}
We have proposed a three-step procedure for using variational autoencoders for decision-making. This is theoretically motivated by analyzing the derived error of the SNIS estimator and the biases of variational Bayes on the pPCA model. Our numerical experiments show that in important real-world examples this three-step procedure outperforms the VAE, IWAE, WW, the $\rchi$-VAE, but also IWAE followed by AIS. %We hope this work will help using VAEs for making scientific discoveries. % sbVAEs are therefore especially suitable for Bayesian decision-making. 

% \cite{NIPS2017_6866} discusses of the numerical stability of the exponentiated CUBO. More recently, \cite{cubo_challenges} found that CUBO might be sensitive to initialization and require an impractical number of samples. We did not encounter such instabilities in our experiments, possibly due to the regularization that comes from amortized inference.

An alternative, and largely orthogonal, approach to decision-making is the elegant framework of loss-calibrated inference~\cite{lacoste2011approximate,NIPS2019_8868}, which further adapts the ELBO to take into account the loss function $L$. Similarly, amortized Monte Carlo integration~\cite{golinski2019amortized} proposes to fit a variational distribution with a model and a target in mind. However, both frameworks are not directly applicable because adapting the ELBO for a specific decision-making loss implies a significant bias in learning $p_\theta$. Still, developing hybrid algorithms is a promising direction for future research.

% Our procedure makes use of both of the forward KL divergence and the $\chi^2$ divergence because of their role in upper bounding the error of the IS estimator. However, the suitability of a given divergence for variational inference depends on the generative model as well as the choice of the variational family. Better performance perhaps could be achieved with a family of divergences that interpolates between forward KL and $\chi^2$, such as the R\`enyi divergence family~\cite{NIPS2016_6208}. Tail adaptive algorithms~\cite{NIPS2018_7816} might help to select the best performing R\`enyi divergence.
